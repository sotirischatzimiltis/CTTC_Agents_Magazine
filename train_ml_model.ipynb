{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ce5c839",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 427228 | Val: 106807\n",
      "Attack rate (train): 0.0127 | (val): 0.0127\n",
      "\n",
      "=== Threat Score (uncalibrated proba) ===\n",
      "ROC AUC:    0.9986799668613267\n",
      "PR AUC:     0.962283079358411\n",
      "Log Loss:   0.02912435662172654\n",
      "Brier:      0.00997746210933732\n",
      "\n",
      "--- Classification report @ 0.5 (reference) ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9999    0.9848    0.9923    105450\n",
      "           1     0.4571    0.9926    0.6259      1357\n",
      "\n",
      "    accuracy                         0.9849    106807\n",
      "   macro avg     0.7285    0.9887    0.8091    106807\n",
      "weighted avg     0.9930    0.9849    0.9877    106807\n",
      "\n",
      "\n",
      "--- Threshold sweep ---\n",
      "t= 0.5 | alerts=  2.759% | TP= 1347 | FN=   10\n",
      "t= 0.2 | alerts=  3.213% | TP= 1353 | FN=    4\n",
      "t= 0.1 | alerts=  3.394% | TP= 1353 | FN=    4\n",
      "t=0.05 | alerts=  3.544% | TP= 1354 | FN=    3\n",
      "t=0.02 | alerts=  3.679% | TP= 1354 | FN=    3\n",
      "t=0.01 | alerts=  3.862% | TP= 1355 | FN=    2\n",
      "\n",
      "--- Score quantiles ---\n",
      "NEG [p50,p90,p99,p99.9,max]: [5.69000e-04 7.31000e-04 7.28559e-01 9.32136e-01 9.99282e-01]\n",
      "POS [p50,p90,p99,p99.9,max]: [0.999282 0.999282 0.99933  0.99933  0.99933 ]\n",
      "\n",
      "✅ Saved Threat Score model: threat_score_model.joblib\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score,\n",
    "    log_loss,\n",
    "    brier_score_loss,\n",
    "    classification_report,\n",
    "    average_precision_score,\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "data_folder = \"Processed_UE_Datasets_unscaled\"\n",
    "\n",
    "label_column = \"binary_label\"   # <-- change if needed\n",
    "feature_columns = [\n",
    "    'epre','pusch_snr','p_ue','ul_mcs','cqi','ul_bitrate',\n",
    "    'dl_mcs','dl_retx','ul_tx','dl_tx','ul_retx','dl_bitrate','dl_err','ul_err'\n",
    "]\n",
    "\n",
    "model_path = \"threat_score_model.joblib\"\n",
    "\n",
    "# -----------------------------\n",
    "# Load TRAIN files only\n",
    "# -----------------------------\n",
    "csv_files = glob.glob(os.path.join(data_folder, \"*.csv\"))\n",
    "train_files = [f for f in csv_files if \"test\" not in os.path.basename(f).lower()]\n",
    "\n",
    "if not train_files:\n",
    "    raise RuntimeError(\"No training files found (no CSVs without 'test' in filename).\")\n",
    "\n",
    "dfs = []\n",
    "for f in train_files:\n",
    "    df = pd.read_csv(f)\n",
    "    missing = [c for c in feature_columns + [label_column] if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"Missing columns {missing} in file: {os.path.basename(f)}\")\n",
    "    dfs.append(df)\n",
    "\n",
    "data = pd.concat(dfs, axis=0).reset_index(drop=True)\n",
    "\n",
    "X = data[feature_columns].copy()\n",
    "y = data[label_column].copy()\n",
    "\n",
    "# Ensure numeric\n",
    "for c in feature_columns:\n",
    "    X[c] = pd.to_numeric(X[c], errors=\"coerce\")\n",
    "y = pd.to_numeric(y, errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "# -----------------------------\n",
    "# Split: train / val\n",
    "# -----------------------------\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(X_train)} | Val: {len(X_val)}\")\n",
    "print(f\"Attack rate (train): {y_train.mean():.4f} | (val): {y_val.mean():.4f}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Train base model (Threat Score)\n",
    "# -----------------------------\n",
    "model = HistGradientBoostingClassifier(\n",
    "    max_depth=10,\n",
    "    learning_rate=0.05,\n",
    "    max_iter=300,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Threat Score = uncalibrated proba for class 1\n",
    "score_val = model.predict_proba(X_val)[:, 1]\n",
    "\n",
    "# -----------------------------\n",
    "# Metrics (imbalance-friendly)\n",
    "# -----------------------------\n",
    "def report_scores(name, probs):\n",
    "    y_true = y_val.values\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"ROC AUC:   \", roc_auc_score(y_true, probs))\n",
    "    print(\"PR AUC:    \", average_precision_score(y_true, probs))\n",
    "    print(\"Log Loss:  \", log_loss(y_true, probs))\n",
    "    print(\"Brier:     \", brier_score_loss(y_true, probs))\n",
    "\n",
    "    # Reference report at 0.5 (usually useless in heavy imbalance, but keep for context)\n",
    "    preds_05 = (probs >= 0.5).astype(int)\n",
    "    print(\"\\n--- Classification report @ 0.5 (reference) ---\")\n",
    "    print(classification_report(y_true, preds_05, digits=4))\n",
    "\n",
    "    # Threshold sweep for operational feel\n",
    "    print(\"\\n--- Threshold sweep ---\")\n",
    "    for t in [0.5, 0.2, 0.1, 0.05, 0.02, 0.01]:\n",
    "        preds = (probs >= t).astype(int)\n",
    "        tp = ((preds == 1) & (y_true == 1)).sum()\n",
    "        fn = ((preds == 0) & (y_true == 1)).sum()\n",
    "        alert_rate = preds.mean() * 100\n",
    "        print(f\"t={t:>4} | alerts={alert_rate:7.3f}% | TP={tp:5d} | FN={fn:5d}\")\n",
    "\n",
    "    # Quick distribution check\n",
    "    pos = probs[y_true == 1]\n",
    "    neg = probs[y_true == 0]\n",
    "    print(\"\\n--- Score quantiles ---\")\n",
    "    if len(neg):\n",
    "        print(\"NEG [p50,p90,p99,p99.9,max]:\", np.round(np.quantile(neg, [0.5, 0.9, 0.99, 0.999, 1.0]), 6))\n",
    "    if len(pos):\n",
    "        print(\"POS [p50,p90,p99,p99.9,max]:\", np.round(np.quantile(pos, [0.5, 0.9, 0.99, 0.999, 1.0]), 6))\n",
    "\n",
    "report_scores(\"Threat Score (uncalibrated proba)\", score_val)\n",
    "\n",
    "# -----------------------------\n",
    "# Save model artifact (for score.py)\n",
    "# -----------------------------\n",
    "joblib.dump(\n",
    "    {\n",
    "        \"model\": model,\n",
    "        \"feature_columns\": feature_columns,\n",
    "        \"label_column\": label_column,\n",
    "        \"model_type\": \"HistGradientBoostingClassifier\",\n",
    "        \"output\": \"threat_score\",\n",
    "        \"calibrated\": False,\n",
    "        \"trained_at_unix\": int(time.time())\n",
    "    },\n",
    "    model_path\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Saved Threat Score model: {model_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02c6b5b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shap311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
